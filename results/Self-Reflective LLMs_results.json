{
  "paper_name": "Self-Reflective LLMs.pdf",
  "processed_date": "2025-06-20T02:49:36.015047",
  "insights": "Title:\nReflexion: Language Agents with Verbal Reinforcement Learning Noah Shinn Northeastern University noahshinn024gmail. com Federico Cassano Northeastern University cassano. fnortheastern. edu Edward Berman Northeastern University berman. ednortheastern. edu Ashwin Gopinath Massachusetts Institute of Technology agopimit. edu Karthik Narasimhan Princeton University karthiknprinceton. edu Shunyu Yao Princeton University shunyuyprinceton. edu Abstract Large language models( LLMs) have been increasingly used to interact with exter- nal environments( e. g. , games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require exten- sive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but in- stead through linguistic feedback.\n\nMain Topic:\nThis paper focuses on Language Agents with Verbal Reinforcement Learning Noah Shinn Northeastern University noahshinn024gmail. com Federico Cassano Northeastern University cassano. fnortheastern. edu Edward Berman Northeastern University berman. ednortheastern. edu Ashwin Gopinath Massachusetts Institute of Technology agopimit. edu Karthik Narasimhan Princeton University karthiknprinceton. edu Shunyu Yao Princeton University shunyuyprinceton. edu Abstract Large language models( LLMs) have been increasingly used to interact with exter- nal environments( e. g. , games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require exten- sive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but in- stead through linguistic feedback..\n\nAbstract Summary:\nConcretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. 1 Introduction Recent works such as ReAct 30, SayCan 1, Toolformer 22, HuggingGPT 23, generative agents 19, and WebGPT 17 have demonstrated the feasibility of autonomous decision-making agents that are built on top of a large language model( LLM) core.\n\nKey Points:\n1. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials.\n2. Reflexion is flexible enough to incorporate various types( scalar values or free-form language) and sources( external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks( sequential decision-making, coding, language reasoning) .\n3. For example, Reflexion achieves a 91 pass1 accuracy on the HumanEval coding benchmark, surpassing the previ- ous state-of-the-art GPT-4 that achieves 80.",
  "hypotheses": "Research Hypotheses:\n\n1. Research exploring Language Agents with Verbal Reinforcement Learning Noah Shinn Northeastern University... could lead to novel approaches that enhance the efficiency and effectiveness of current methodologies.\n\n2. Building upon the finding that Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials., we hypothesize that integrating these insights with emerging technologies could lead to breakthrough applications.\n\n3. Based on the observation that Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.\n\n4. Based on the observation that Reflexion is flexible enough to incorporate various types( scalar values or free-form language) and sources( external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks( sequential decision-making, coding, language reasoning) ., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.",
  "num_chunks": 64,
  "text_length": 58935
}