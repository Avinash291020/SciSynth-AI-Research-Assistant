{
  "paper_name": "Tree of Thoughts Deliberate Problem Solving with LLMs.pdf",
  "processed_date": "2025-06-20T02:49:37.703179",
  "insights": "Title:\nTree of Thoughts: Deliberate Problem Solving with Large Language Models Shunyu Yao Princeton University Dian Yu Google DeepMind Jeffrey Zhao Google DeepMind Izhak Shafran Google DeepMind Thomas L. Griffiths Princeton University Yuan Cao Google DeepMind Karthik Narasimhan Princeton University Abstract Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts( ToT) , which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text( thoughts) that serve as intermediate steps toward problem solving.\n\nMain Topic:\nThis paper focuses on Deliberate Problem Solving with Large Language Models Shunyu Yao Princeton University Dian Yu Google DeepMind Jeffrey Zhao Google DeepMind Izhak Shafran Google DeepMind Thomas L. Griffiths Princeton University Yuan Cao Google DeepMind Karthik Narasimhan Princeton University Abstract Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts( ToT) , which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text( thoughts) that serve as intermediate steps toward problem solving..\n\nAbstract Summary:\nToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion.\n\nKey Points:\n1. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.\n2. Our experiments show that ToT significantly enhances language models problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\n3. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4 of tasks, our method achieved a success rate of 74.",
  "hypotheses": "Research Hypotheses:\n\n1. Research exploring Deliberate Problem Solving with Large Language Models Shunyu Yao Princeton... could lead to novel approaches that enhance the efficiency and effectiveness of current methodologies.\n\n2. Building upon the finding that ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices., we hypothesize that integrating these insights with emerging technologies could lead to breakthrough applications.\n\n3. Based on the observation that ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.\n\n4. Based on the observation that Our experiments show that ToT significantly enhances language models problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.",
  "num_chunks": 57,
  "text_length": 54436
}