{
  "paper_name": "Chain-of-Thought Prompting Elicits Reasoning in LLMs.pdf",
  "processed_date": "2025-06-20T02:49:32.702097",
  "insights": "Title:\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V. Le Denny Zhou Google Research, Brain Team jasonwei, dennyzhougoogle. com Abstract We explore how generating a chain of thoughta series of intermediate reasoning stepssigniﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking.\n\nMain Topic:\nThis paper focuses on Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma Brian Ichter Fei Xia Ed H. Chi Quoc V. Le Denny Zhou Google Research, Brain Team jasonwei, dennyzhougoogle. com Abstract We explore how generating a chain of thoughta series of intermediate reasoning stepssigniﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-of- thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking..\n\nAbstract Summary:\nFor instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n\nKey Points:\n1. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.\n2. Chain-of-Thought Prompting Q: Roger has 5 tennis balls.\n3. If they used 20 to make lunch and bought 6 more, how many apples do they have?",
  "hypotheses": "Research Hypotheses:\n\n1. Research exploring Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Jason Wei... could lead to novel approaches that enhance the efficiency and effectiveness of current methodologies.\n\n2. Building upon the finding that For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer., we hypothesize that integrating these insights with emerging technologies could lead to breakthrough applications.\n\n3. Based on the observation that For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.\n\n4. Based on the observation that Chain-of-Thought Prompting Q: Roger has 5 tennis balls., we hypothesize that further investigation could reveal new theoretical frameworks and practical applications.",
  "num_chunks": 145,
  "text_length": 136047
}