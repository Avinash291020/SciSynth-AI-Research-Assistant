"""Streamlit UI for SciSynth Research Paper Analysis."""
import os
import sys

# Add parent directory to Python path
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

import streamlit as st
import json
from pathlib import Path
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from app.batch_processor import BatchProcessor
from app.research_paper_processor import ResearchPaperProcessor
from app.rag_system import RAGSystem
from app.rl_selector import RLPaperRecommender
from evolutionary.evolve_hypotheses import EvolutionaryHypothesisGenerator
from agents.orchestrator import ResearchOrchestrator

# Page config
st.set_page_config(
    page_title="SciSynth: AI Research Assistant",
    page_icon="üß™",
    layout="wide"
)

# Custom CSS for professional styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
        margin: 0.5rem 0;
    }
    .success-box {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .info-box {
        background: #d1ecf1;
        border: 1px solid #bee5eb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .sidebar .sidebar-content {
        background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
    }
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background: linear-gradient(90deg, #5a6fd8 0%, #6a4190 100%);
        transform: translateY(-2px);
        transition: all 0.3s ease;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'processor' not in st.session_state:
    st.session_state.processor = None
    st.session_state.processor_initialized = False

if 'results_loaded' not in st.session_state:
    st.session_state.results_loaded = False
if 'orchestrator' not in st.session_state:
    st.session_state.orchestrator = None

# --- Singleton Orchestrator Loader ---
@st.cache_resource
def get_orchestrator(papers):
    return ResearchOrchestrator(papers)

# Load results if available
results_path = Path("results/all_papers_results.json")
if results_path.exists():
    try:
        with open(results_path, 'r', encoding='utf-8') as f:
            all_results = json.load(f)
        st.session_state.results_loaded = True
        st.session_state.all_results = all_results

        # Singleton orchestrator: only load if not present or papers changed
        if (
            'orchestrator' not in st.session_state or
            st.session_state.orchestrator is None or
            getattr(st.session_state.orchestrator, 'papers', None) != all_results
        ):
            st.session_state.orchestrator = get_orchestrator(all_results)
    except Exception as e:
        st.error(f"Error loading results: {str(e)}")

# --- ADVANCED SIDEBAR NAVIGATION ---
st.sidebar.title("üß™ SciSynth AI - Advanced Dashboard")
st.sidebar.markdown("**AI Research Assistant (Professional Edition)**")

# Add system status to sidebar
if st.session_state.orchestrator:
    st.sidebar.success("‚úÖ System Ready")
    status = st.session_state.orchestrator.get_system_status()
    st.sidebar.metric("Papers Loaded", status["total_papers"])
    st.sidebar.metric("Active Systems", len([s for s in status["systems_status"].values() if s == "ready"]))
else:
    st.sidebar.warning("‚ö†Ô∏è System Not Ready")
    st.sidebar.info("Process research collection to enable all features")

main_pages = [
    "üè† Overview & AI Stack",
    "üìÑ Individual Paper Analysis",
    "üìö Research Collection Analysis",
    "üîó Citation Network Analysis",
    "üìä Topic Analysis",
    "ü§ñ LLM & RAG (Retrieval-Augmented Generation)",
    "üéØ RL (Reinforcement Learning)",
    "üß¨ Evolutionary AI",
    "üß† Agentic AI & Planning",
    "üî¢ Symbolic & Neuro-Symbolic AI",
    "üéº AI Orchestrator (All Systems)",
    "üìã Capabilities Dashboard"
]

page = st.sidebar.selectbox("Choose Section", main_pages)

# --- Helper: Project-Specific AI Paradigm Explanations ---
def get_paradigm_explanation(paradigm):
    explanations = {
        "Generative AI": '''
**How Generative AI is used in SciSynth:**\n\n- Implements T5-based models for hypothesis and insight generation.\n- Automated research summaries and synthesis using `app/hypothesis_gen.py` and `app/insight_agent.py`.\n- Model management via `app/model_cache.py`.\n- Models: google/flan-t5-base, Sentence Transformers.\n''',
        "Agentic AI": '''
**How Agentic AI is used in SciSynth:**\n\n- Autonomous research task planning via `agents/cognitive_planner.py`.\n- System coordination and task execution via `agents/orchestrator.py`.\n- Multi-agent architecture with specialized roles.\n- Features: goal-driven paper analysis, multi-step workflows, intelligent task prioritization.\n- Each research goal is broken into steps (literature review, gap analysis, hypothesis generation, paper analysis, synthesis), each executed by the most appropriate AI tool.\n''',
        "RAG": '''
**How RAG is used in SciSynth:**\n\n- Complete RAG pipeline in `app/rag_system.py`.\n- ChromaDB for vector storage, Sentence Transformers for embeddings, T5 for answer generation.\n- Semantic paper retrieval, contextual Q&A, multi-document synthesis, relevance scoring.\n''',
        "Symbolic AI": '''
**How Symbolic AI is used in SciSynth:**\n\n- Citation extraction and analysis via `app/citation_network.py`.\n- Logical consistency validation via `logic/consistency_checker.py`.\n- Prolog-based rule system in `logic/symbolic_rules.pl`.\n- Graph-based relationship analysis with NetworkX.\n''',
        "Neuro-Symbolic AI": '''
**How Neuro-Symbolic AI is used in SciSynth:**\n\n- Combines neural embeddings (Sentence Transformers) with symbolic rules (Prolog).\n- Citation network analysis with neural similarity.\n- Hybrid hypothesis generation and multi-modal research synthesis.\n''',
        "Machine Learning": '''
**How Machine Learning is used in SciSynth:**\n\n- ML model training and evaluation in `app/model_tester.py`.\n- scikit-learn for traditional ML algorithms.\n- Feature extraction and performance metrics from research papers.\n''',
        "Deep Learning": '''
**How Deep Learning is used in SciSynth:**\n\n- PyTorch and Transformers library for deep learning.\n- Sentence Transformers for neural embeddings.\n- T5 models for text generation and understanding.\n''',
        "Reinforcement Learning": '''
**How Reinforcement Learning is used in SciSynth:**\n\n- RL recommendation system in `app/rl_selector.py`.\n- DQN (Deep Q-Network) agent for paper recommendation.\n- Paper environment simulation and reward-based learning.\n''',
        "Evolutionary Algorithms": '''
**How Evolutionary Algorithms are used in SciSynth:**\n\n- Complete evolutionary system in `evolutionary/evolve_hypotheses.py`.\n- DEAP framework for distributed evolutionary algorithms.\n- Genetic operators: crossover, mutation, selection.\n- Fitness evaluation based on research relevance.\n''',
        "LLM": '''
**How LLMs are used in SciSynth:**\n\n- T5 models as the large language model backbone.\n- Local deployment of google/flan-t5-base.\n- Text generation pipelines and contextual understanding.\n''',
    }
    return explanations.get(paradigm, "No project-specific explanation available.")

# --- LANDING/OVERVIEW PAGE ---
if page == "üè† Overview & AI Stack":
    st.markdown('<div class="main-header"><h1>SciSynth: AI Research Assistant (Professional Edition)</h1></div>', unsafe_allow_html=True)
st.markdown("""
    Welcome to the **Advanced AI Research Assistant**. This dashboard demonstrates a full-stack, multi-paradigm AI system for scientific literature analysis, synthesis, and discovery.
    
    **AI Paradigms Implemented:**
    - üß† LLMs & Generative AI
    - ü§ñ RAG (Retrieval-Augmented Generation)
    - üéØ Reinforcement Learning
    - üß¨ Evolutionary Algorithms
    - üî¢ Symbolic & Neuro-Symbolic AI
    - üß† Agentic AI (Cognitive Planning)
    - üìä Machine Learning & Deep Learning
    
    **What makes this advanced?**
    - All paradigms are fully integrated and orchestrated
    - Each module exposes detailed outputs, visualizations, and explanations
    - Professional UI/UX for research and demo purposes
    """)
    
    # Show citation network if available
    if Path("results/citation_network.png").exists():
        st.image("results/citation_network.png", caption="Citation Network Example", use_container_width=True)
    
    st.markdown('<div class="info-box">Explore each section from the sidebar to see advanced AI in action.</div>', unsafe_allow_html=True)
    st.markdown("---")
    st.subheader("AI Capabilities Matrix")
    if 'orchestrator' in st.session_state and st.session_state.orchestrator:
        capabilities = st.session_state.orchestrator.get_ai_capabilities_summary()
        summary_data = []
        for ai_type, details in capabilities.items():
            summary_data.append({
                "AI Type": ai_type.upper(),
                "Status": details["status"],
                "Capabilities": ", ".join(details["capabilities"]),
                "Implementation": "‚úÖ Complete"
            })
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True)
        
        # Add metrics dashboard
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Papers", len(st.session_state.all_results))
        with col2:
            papers_with_insights = sum(1 for p in st.session_state.all_results if p.get('insights'))
            st.metric("Papers with Insights", papers_with_insights)
        with col3:
            papers_with_hypotheses = sum(1 for p in st.session_state.all_results if p.get('hypotheses'))
            st.metric("Papers with Hypotheses", papers_with_hypotheses)
        with col4:
            active_systems = len([s for s in capabilities.values() if s["status"] == "active"])
            st.metric("Active AI Systems", active_systems)
    else:
        st.markdown('<div class="warning-box">Orchestrator not initialized. Process research collection to enable full dashboard.</div>', unsafe_allow_html=True)
        st.warning("Orchestrator not initialized. Process research collection to enable full dashboard.")

# --- INDIVIDUAL PAPER ANALYSIS ---
elif page == "üìÑ Individual Paper Analysis":
    st.header("üìÑ Individual Paper Analysis")
    st.markdown("**AI Paradigm: LLM + Generative AI**")
    
    # Initialize processor only when needed
    if st.session_state.processor is None and not st.session_state.processor_initialized:
        try:
            # Set environment variable to avoid PyTorch device issues
            os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU usage
            st.session_state.processor = ResearchPaperProcessor()
            st.session_state.processor_initialized = True
            st.success("‚úÖ Paper processor initialized successfully!")
        except Exception as e:
            st.error(f"‚ùå Error initializing processor: {str(e)}")
            st.session_state.processor_initialized = True  # Mark as attempted
    
    if st.session_state.processor is None:
        st.error("‚ùå Paper processor not available. Please restart the app.")
        if st.button("üîÑ Retry Initialization"):
            try:
                st.session_state.processor = ResearchPaperProcessor()
                st.session_state.processor_initialized = True
                st.success("‚úÖ Processor initialized successfully!")
                st.rerun()
            except Exception as e:
                st.error(f"‚ùå Failed to initialize: {str(e)}")
    else:
    uploaded_file = st.file_uploader("Upload a research paper (PDF)", type="pdf")
    
    if uploaded_file:
        with st.spinner("Processing paper..."):
                try:
            # Save uploaded file
            temp_path = Path("temp") / uploaded_file.name
            temp_path.parent.mkdir(exist_ok=True)
            temp_path.write_bytes(uploaded_file.getvalue())
            
            # Process paper
            result = st.session_state.processor.process_paper(temp_path)
            
            if result:
                # Display results in tabs
                tab1, tab2, tab3 = st.tabs(["üìä Overview", "üí° Insights", "üî¨ Hypotheses"])
                
                with tab1:
                    st.subheader("Paper Overview")
                    metadata = result["metadata"]
                            
                            # Display title with better formatting
                            title = metadata.get('title', 'Title not extracted')
                            if title:
                                st.markdown(f"**üìÑ Title:** {title}")
                            else:
                                st.warning("‚ö†Ô∏è Title could not be extracted from the PDF")
                            
                            # Display date with better formatting
                            date = metadata.get('date', 'Not available')
                            if date and date != 'Not available':
                                # Try to parse and format the date
                                try:
                                    if date.startswith('D:'):
                                        # PDF date format: D:YYYYMMDDHHmmSSOHH'mm'
                                        date_str = date[2:10]  # Extract YYYYMMDD
                                        formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                                        st.markdown(f"**üìÖ Date:** {formatted_date}")
                                    else:
                                        st.markdown(f"**üìÖ Date:** {date}")
                                except:
                                    st.markdown(f"**üìÖ Date:** {date}")
                            else:
                                st.markdown("**üìÖ Date:** Not available")
                            
                            # Display keywords if available
                    if metadata.get('keywords'):
                                st.markdown("**üîë Keywords:** " + ", ".join(metadata['keywords']))
                    
                            # Technical terms with better formatting
                    if metadata.get('technical_terms'):
                                st.subheader("üî¨ Technical Terms & Abbreviations")
                                terms = metadata['technical_terms']
                                if len(terms) > 0:
                                    # Group terms by type
                                    abbreviations = [t for t in terms if len(t) <= 10 and t.isupper()]
                                    algorithms = [t for t in terms if 'algorithm' in t.lower() or 'algo' in t.lower()]
                                    models = [t for t in terms if 'model' in t.lower()]
                                    frameworks = [t for t in terms if 'framework' in t.lower()]
                                    other_terms = [t for t in terms if t not in abbreviations + algorithms + models + frameworks]
                                    
                                    if abbreviations:
                                        st.markdown("**Abbreviations:**")
                                        for abbr in abbreviations[:10]:  # Limit to first 10
                                            st.markdown(f"  ‚Ä¢ {abbr}")
                                    
                                    if algorithms:
                                        st.markdown("**Algorithms:**")
                                        for algo in algorithms[:5]:
                                            st.markdown(f"  ‚Ä¢ {algo}")
                                    
                                    if models:
                                        st.markdown("**Models:**")
                                        for model in models[:5]:
                                            st.markdown(f"  ‚Ä¢ {model}")
                                    
                                    if frameworks:
                                        st.markdown("**Frameworks:**")
                                        for framework in frameworks[:5]:
                                            st.markdown(f"  ‚Ä¢ {framework}")
                                    
                                    if other_terms:
                                        st.markdown("**Other Technical Terms:**")
                                        for term in other_terms[:10]:
                                            st.markdown(f"  ‚Ä¢ {term}")
                                    
                                    # Show total count
                                    st.info(f"üìä Total technical terms extracted: {len(terms)}")
                                else:
                                    st.info("No technical terms found in this paper.")
                            else:
                                st.info("No technical terms extracted from this paper.")
                
                with tab2:
                    st.subheader("Generated Insights")
                    insights = result["insights"].split("\n")
                    for insight in insights:
                        if insight.strip():
                            st.markdown(f"- {insight}")
                
                with tab3:
                    st.subheader("Research Hypotheses")
                    hypotheses = result["hypotheses"].split("\n")
                    for hypothesis in hypotheses:
                        if hypothesis.strip() and not hypothesis.startswith("Research Hypotheses:"):
                            st.markdown(f"- {hypothesis}")
                    else:
                        st.error("Failed to process the paper. Please try again.")
                except Exception as e:
                    st.error(f"Error processing paper: {str(e)}")

# --- RESEARCH COLLECTION ANALYSIS ---
elif page == "üìö Research Collection Analysis":
    st.header("üìö Research Collection Analysis")
    st.markdown("**AI Paradigm: ML + DL**")
    
    # Load results
    results_path = Path("results/all_papers_results.json")
    if results_path.exists():
        try:
            with open(results_path, encoding='utf-8') as f:
            all_results = json.load(f)
        st.session_state.results_loaded = True
        
        # Summary statistics
        st.subheader("Collection Overview")
            col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Papers", len(all_results))
        with col2:
            avg_chunks = sum(r["num_chunks"] for r in all_results) / len(all_results)
            st.metric("Avg. Chunks per Paper", f"{avg_chunks:.1f}")
        with col3:
            total_text = sum(r["text_length"] for r in all_results)
            st.metric("Total Text Analyzed", f"{total_text/1000:.1f}K chars")
            with col4:
                papers_with_metadata = sum(1 for r in all_results if r.get("metadata", {}).get("title"))
                st.metric("Papers with Metadata", papers_with_metadata)
            
            # Enhanced paper list with filters
            st.subheader("üìÑ Enhanced Paper List")
            
            # Create filters
            col1, col2, col3 = st.columns(3)
            with col1:
                # Paper type filter
                paper_types = list(set(r.get("metadata", {}).get("paper_type", "unknown") for r in all_results))
                selected_type = st.selectbox("Filter by Paper Type", ["All"] + paper_types)
            
            with col2:
                # Year filter
                years = list(set(r.get("metadata", {}).get("date", "") for r in all_results if r.get("metadata", {}).get("date")))
                years = [y for y in years if y and y.isdigit()]
                years.sort(reverse=True)
                selected_year = st.selectbox("Filter by Year", ["All"] + years)
            
            with col3:
                # Author filter
                all_authors = []
                for r in all_results:
                    authors = r.get("metadata", {}).get("authors", [])
                    all_authors.extend(authors)
                unique_authors = list(set(all_authors))
                unique_authors.sort()
                selected_author = st.selectbox("Filter by Author", ["All"] + unique_authors[:20])  # Limit to first 20
            
            # Filter papers
            filtered_results = all_results
            if selected_type != "All":
                filtered_results = [r for r in filtered_results if r.get("metadata", {}).get("paper_type") == selected_type]
            if selected_year != "All":
                filtered_results = [r for r in filtered_results if r.get("metadata", {}).get("date") == selected_year]
            if selected_author != "All":
                filtered_results = [r for r in filtered_results if selected_author in r.get("metadata", {}).get("authors", [])]
            
            # Create enhanced dataframe
            papers_data = []
            for r in filtered_results:
                metadata = r.get("metadata", {})
                paper_data = {
                    "Title": metadata.get("title", r["paper_name"].replace(".pdf", "")),
                    "Authors": ", ".join(metadata.get("authors", [])[:3]) if metadata.get("authors") else "N/A",
                    "Affiliations": ", ".join(metadata.get("affiliations", [])[:2]) if metadata.get("affiliations") else "N/A",
                    "Type": metadata.get("paper_type", "unknown"),
                    "Date": metadata.get("date", "N/A"),
                    "Keywords": ", ".join(metadata.get("keywords", [])[:3]) if metadata.get("keywords") else "N/A",
                    "Technical Terms": len(metadata.get("technical_terms", [])),
                    "Sections": len(metadata.get("sections", {})),
                    "References": len(metadata.get("references", [])),
                    "DOIs": len(metadata.get("dois", [])),
                    "URLs": len(metadata.get("urls", []))
                }
                papers_data.append(paper_data)
            
            papers_df = pd.DataFrame(papers_data)
        st.dataframe(papers_df, use_container_width=True)
        
            # Show detailed paper information in expandable sections
            st.subheader("üìã Detailed Paper Information")
            for i, paper in enumerate(filtered_results[:5]):  # Show first 5 papers in detail
                metadata = paper.get("metadata", {})
                with st.expander(f"üìÑ {metadata.get('title', paper['paper_name'])}", expanded=False):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Basic Information:**")
                        st.write(f"**Title:** {metadata.get('title', 'N/A')}")
                        st.write(f"**Paper Type:** {metadata.get('paper_type', 'N/A')}")
                        st.write(f"**Date:** {metadata.get('date', 'N/A')}")
                        
                        if metadata.get('authors'):
                            st.write("**Authors:**")
                            for author in metadata['authors'][:5]:
                                st.write(f"  ‚Ä¢ {author}")
                        
                        if metadata.get('affiliations'):
                            st.write("**Affiliations:**")
                            for affil in metadata['affiliations'][:3]:
                                st.write(f"  ‚Ä¢ {affil}")
                    
                    with col2:
                        st.write("**Content Analysis:**")
                        st.write(f"**Keywords:** {', '.join(metadata.get('keywords', [])[:5])}")
                        st.write(f"**Technical Terms:** {len(metadata.get('technical_terms', []))}")
                        st.write(f"**Sections:** {len(metadata.get('sections', {}))}")
                        st.write(f"**References:** {len(metadata.get('references', []))}")
                        
                        if metadata.get('dois'):
                            st.write("**DOIs:**")
                            for doi in metadata['dois'][:2]:
                                st.write(f"  ‚Ä¢ {doi}")
                        
                        if metadata.get('urls'):
                            st.write("**URLs:**")
                            for url in metadata['urls'][:2]:
                                st.write(f"  ‚Ä¢ {url}")
                    
                    # Show sections if available
                    if metadata.get('sections'):
                        st.write("**Sections:**")
                        for section_name, section_text in list(metadata['sections'].items())[:3]:
                            with st.expander(f"üìù {section_name}", expanded=False):
                                st.write(section_text[:500] + "..." if len(section_text) > 500 else section_text)
            
            # Analytics section
            st.subheader("üìä Collection Analytics")
            
            # Paper type distribution
            col1, col2 = st.columns(2)
            with col1:
                paper_type_counts = {}
                for r in all_results:
                    ptype = r.get("metadata", {}).get("paper_type", "unknown")
                    paper_type_counts[ptype] = paper_type_counts.get(ptype, 0) + 1
                
                if paper_type_counts:
                    st.write("**Paper Type Distribution:**")
                    for ptype, count in paper_type_counts.items():
                        st.write(f"‚Ä¢ {ptype.title()}: {count} papers")
            
            with col2:
                # Top authors
                author_counts = {}
                for r in all_results:
                    authors = r.get("metadata", {}).get("authors", [])
                    for author in authors:
                        author_counts[author] = author_counts.get(author, 0) + 1
                
                if author_counts:
                    st.write("**Top Authors:**")
                    top_authors = sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                    for author, count in top_authors:
                        st.write(f"‚Ä¢ {author}: {count} papers")
            
        # Download options
            st.subheader("üíæ Download Options")
            col1, col2 = st.columns(2)
            with col1:
        st.download_button(
            "Download Full Analysis (JSON)",
            data=json.dumps(all_results, indent=2),
            file_name="research_collection_analysis.json",
            mime="application/json"
        )
            with col2:
                st.download_button(
                    "Download Enhanced Metadata (CSV)",
                    data=papers_df.to_csv(index=False),
                    file_name="research_collection_metadata.csv",
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"Error loading results: {str(e)}")
    else:
        st.warning("No research collection analysis found. Process papers first.")
        if st.button("Process Research Collection"):
            with st.spinner("Processing all papers..."):
                try:
                processor = BatchProcessor()
                results = processor.process_all_papers()
                st.success(f"Processed {len(results)} papers successfully!")
                    st.rerun()
                except Exception as e:
                    st.error(f"Error processing collection: {str(e)}")

# --- CITATION NETWORK ANALYSIS ---
elif page == "üîó Citation Network Analysis":
    st.header("üîó Citation Network Analysis")
    st.markdown("**AI Paradigm: Symbolic + Neuro-Symbolic**")
    
    if st.session_state.results_loaded:
        # Load network analysis
        network_path = Path("results/network_analysis.json")
        if network_path.exists():
            try:
                with open(network_path, encoding='utf-8') as f:
                network_data = json.load(f)
            
            # Network statistics
            st.subheader("Network Overview")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Papers", network_data["num_papers"])
            with col2:
                st.metric("Citations", network_data["num_citations"])
            with col3:
                st.metric("Relationships", network_data["num_relationships"])
            
            # Citation network visualization
            st.subheader("Citation Network Graph")
            network_img = Path("results/citation_network.png")
            if network_img.exists():
                st.image(str(network_img))
            
            # Cross-references analysis
            if "cross_references" in network_data:
                st.subheader("Cross-References Analysis")
                
                # Direct citations
                if network_data["cross_references"]["direct_citations"]:
                    st.markdown("**Direct Citations**")
                    citations_df = pd.DataFrame(network_data["cross_references"]["direct_citations"])
                    st.dataframe(citations_df)
                
                # Shared topics
                if network_data["cross_references"]["shared_topics"]:
                    st.markdown("**Papers with Shared Topics**")
                    topics_df = pd.DataFrame(network_data["cross_references"]["shared_topics"])
                    st.dataframe(topics_df)
            except Exception as e:
                st.error(f"Error loading network analysis: {str(e)}")
        else:
            st.warning("Network analysis not found. Process the research collection first.")
    else:
        st.warning("Load the research collection first.")

# --- TOPIC ANALYSIS ---
elif page == "üìä Topic Analysis":
    st.header("üìä Topic Analysis")
    st.markdown("**AI Paradigm: Unsupervised ML**")
    
    if st.session_state.results_loaded:
        try:
        results_path = Path("results/all_papers_results.json")
            with open(results_path, encoding='utf-8') as f:
            all_results = json.load(f)
        
        # Extract and analyze topics
        topics = {}
        for paper in all_results:
            if "metadata" in paper and "keywords" in paper["metadata"]:
                for keyword in paper["metadata"]["keywords"]:
                    topics[keyword] = topics.get(keyword, 0) + 1
        
        # Topic distribution
        st.subheader("Topic Distribution")
            topic_df = pd.DataFrame(list(topics.items()), columns=pd.Index(["Topic", "Count"]))
        topic_df = topic_df.sort_values("Count", ascending=False)
        
        fig = px.bar(topic_df, x="Topic", y="Count",
                    title="Research Topics Frequency")
        st.plotly_chart(fig)
        
        # Topic co-occurrence
        st.subheader("Topic Co-occurrence")
        cooc_matrix = {}
        for paper in all_results:
            if "metadata" in paper and "keywords" in paper["metadata"]:
                keywords = paper["metadata"]["keywords"]
                for i, k1 in enumerate(keywords):
                    for k2 in keywords[i+1:]:
                        pair = tuple(sorted([k1, k2]))
                        cooc_matrix[pair] = cooc_matrix.get(pair, 0) + 1
        
        if cooc_matrix:
            cooc_df = pd.DataFrame([(k[0], k[1], v) for k, v in cooc_matrix.items()],
                                     columns=pd.Index(["Topic 1", "Topic 2", "Co-occurrences"]))
            cooc_df = cooc_df.sort_values("Co-occurrences", ascending=False)
            st.dataframe(cooc_df)
        except Exception as e:
            st.error(f"Error in topic analysis: {str(e)}")
    else:
        st.warning("Load the research collection first.")

# --- LLM & RAG SECTION ---
if page == "ü§ñ LLM & RAG (Retrieval-Augmented Generation)":
    st.header("ü§ñ LLM & RAG: Advanced Document Understanding")
    st.markdown("""
    This module uses **Retrieval-Augmented Generation** (RAG) and Large Language Models (LLMs) to answer questions, summarize documents, and synthesize knowledge from your research collection.
    """)
    if st.session_state.orchestrator:
        # Create a form for better input handling
        with st.form("rag_search_form"):
            question = st.text_input("Ask a research question:", placeholder="e.g., What are the main trends in neuro-symbolic AI?")
            top_k = st.slider("Number of relevant documents to retrieve", 1, 10, 5)
            submitted = st.form_submit_button("üîç Run Advanced RAG/LLM")
        
        if submitted:
            if question.strip():
                with st.spinner("Retrieving and synthesizing answer..."):
                    try:
                        result = st.session_state.orchestrator.rag_system.answer_question(question, top_k=top_k)
                        
                        st.subheader("ü§ñ Synthesized Answer")
                        # Format the response better
                        if result["answer"]:
                            # Clean up repetitive text
                            answer = result["answer"]
                            if answer.count("Key points:") > 2:
                                # Replace repetitive text with a cleaner version
                                answer = answer.replace("Key points: Key points:", "Key points:")
                                answer = answer.replace("Key points: Key points: Key points:", "Key points:")
                            
                            st.markdown(answer)
                        else:
                            st.warning("No answer generated. Please try rephrasing your question.")
                        
                        st.subheader("üìö Retrieved Documents & Context")
                        if result["relevant_papers"]:
                            for i, paper in enumerate(result["relevant_papers"], 1):
                                paper_title = paper['metadata'].get('title', paper['metadata'].get('paper_name', f'Document {i}'))
                                with st.expander(f"Document {i}: {paper_title}"):
                                    st.write(f"**Paper Name:** {paper['metadata'].get('paper_name', 'N/A')}")
                                    if paper['content'] and paper['content'].strip():
                                        # Clean up content display
                                        content = paper['content']
                                        if content.count("Key points:") > 1:
                                            # Show only first occurrence
                                            first_key_points = content.find("Key points:")
                                            if first_key_points > 0:
                                                content = content[:first_key_points + 50] + "..."
                                        
                                        # Show first 200 characters
                                        content_preview = content[:200] + "..." if len(content) > 200 else content
                                        st.write(f"**Content Preview:** {content_preview}")
                                    else:
                                        st.write("**Content:** No content available")
                                    
                                    if paper.get('distance') is not None:
                                        st.write(f"**Relevance Score:** {paper['distance']:.3f}")
                        else:
                            st.warning("No relevant documents found.")
                        
                        st.success(f"‚úÖ Retrieved {result['num_papers_retrieved']} relevant documents.")
                    except Exception as e:
                        st.error(f"Error during RAG processing: {str(e)}")
                        st.info("Try rephrasing your question or reducing the number of documents to retrieve.")
            else:
                st.warning("Please enter a research question.")
        
        st.markdown("---")
        with st.expander("‚ÑπÔ∏è How does RAG/LLM work in SciSynth?", expanded=False):
            st.markdown(get_paradigm_explanation("RAG") + "\n---\n" + get_paradigm_explanation("LLM"))
    else:
        st.warning("Orchestrator not initialized. Process research collection first.")

# --- RL SECTION ---
if page == "üéØ RL (Reinforcement Learning)":
    st.header("üéØ Reinforcement Learning: Paper Recommendation")
    st.markdown("""
    This module uses a Deep Q-Network (DQN) agent to recommend the most relevant and novel research papers based on your collection.
    """)
    if st.session_state.orchestrator:
        col1, col2 = st.columns([2, 1])
        with col1:
            num_recommendations = st.slider("Number of recommendations", 3, 10, 5)
            if st.button("üéØ Get RL Recommendations"):
                with st.spinner("Training RL agent and generating recommendations..."):
                    try:
                        training_result = st.session_state.orchestrator.train_rl_system(episodes=30)
                        if training_result["training_completed"]:
                            st.success("‚úÖ RL agent trained successfully!")
                            recommendations = st.session_state.orchestrator.rl_recommender.recommend_papers(num_recommendations=num_recommendations)
                            st.subheader("üéØ Recommended Papers")
                            for i, rec in enumerate(recommendations, 1):
                                with st.expander(f"{i}. {rec['paper_name']} (Confidence: {rec['confidence']:.3f})"):
                                    st.write(f"**Title:** {rec['title']}")
                                    st.write(f"**Confidence Score:** {rec['confidence']:.3f}")
                                    st.write(f"**Action ID:** {rec['action_id']}")
                        else:
                            st.error("Failed to train RL agent")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
        with col2:
            st.subheader("üìä RL Training Progress")
            try:
                history_path = Path("models/rl_paper_recommender_history.json")
                if history_path.exists():
                    with open(history_path, 'r', encoding='utf-8') as f:
                        history = json.load(f)
                    fig = px.line(y=history, title="RL Training Progress")
                    st.plotly_chart(fig)
                else:
                    st.info("No training history available.")
            except Exception as e:
                st.error(f"Error loading training history: {str(e)}")
        st.markdown("---")
        with st.expander("‚ÑπÔ∏è How does RL Recommendation work in SciSynth?", expanded=False):
            st.markdown(get_paradigm_explanation("Reinforcement Learning"))
    else:
        st.warning("Orchestrator not initialized. Process research collection first.")

# --- EVOLUTIONARY AI SECTION ---
if page == "üß¨ Evolutionary AI":
    st.header("üß¨ Evolutionary Algorithms: Hypothesis Generation & Optimization")
    st.markdown("""
    This module uses genetic algorithms to generate and optimize research hypotheses based on your collection.
    """)
    if st.session_state.orchestrator:
        col1, col2 = st.columns([2, 1])
        with col1:
            num_hypotheses = st.slider("Number of hypotheses to generate", 5, 20, 10)
            generations = st.slider("Number of generations", 10, 50, 20)
            if st.button("üß¨ Generate Hypotheses (Evolutionary)"):
                with st.spinner("Running evolutionary algorithm..."):
                    try:
                        hypotheses = st.session_state.orchestrator.evo_generator.generate_diverse_hypotheses(num_hypotheses=num_hypotheses)
                        st.subheader("üß¨ Generated Hypotheses")
                        for i, hyp in enumerate(hypotheses, 1):
                            with st.expander(f"Hypothesis {i} (Fitness: {hyp['fitness']:.3f})"):
                                st.write(f"**Hypothesis:** {hyp['hypothesis']}")
                                st.write(f"**Fitness Score:** {hyp['fitness']:.3f}")
                                st.write(f"**Rank:** {hyp['rank']}")
                                st.write(f"**Components:** {hyp['components']}")
                        st.success("‚úÖ Hypotheses generated and displayed above.")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
        with col2:
            st.subheader("üß¨ Evolutionary Algorithm Info")
            st.write("**Framework:** DEAP (Distributed Evolutionary Algorithms)")
            st.write("**Selection:** Tournament Selection")
            st.write("**Crossover:** Two-point Crossover")
            st.write("**Mutation:** Custom String Replacement")
            st.write("**Fitness:** Relevance to research papers")
        st.markdown("---")
        with st.expander("‚ÑπÔ∏è How does Evolutionary Hypothesis Generation work in SciSynth?", expanded=False):
            st.markdown(get_paradigm_explanation("Evolutionary Algorithms"))
    else:
        st.warning("Orchestrator not initialized. Process research collection first.")

# --- AGENTIC AI SECTION ---
if page == "üß† Agentic AI & Planning":
    st.header("üß† Agentic AI: Autonomous Cognitive Planning")
    st.markdown("""
    This module uses an autonomous cognitive planner to break down research goals into actionable steps, execute them, and synthesize results using all available AI tools.
    """)
    if st.session_state.orchestrator:
        research_goal = st.text_area("Enter your research goal:", placeholder="e.g., Identify the most promising directions for AI research in the next 5 years", height=100)
        if st.button("üß† Plan & Execute Research Task"):
            if research_goal:
                with st.spinner("Planning and executing research task..."):
                    try:
                        # --- Caching: Store last result in session state to avoid recomputation ---
                        cache_key = f"agentic_result_{research_goal}"
                        if cache_key in st.session_state:
                            result = st.session_state[cache_key]
                        else:
                            result = st.session_state.orchestrator.cognitive_planner.plan_research_task(research_goal)
                            st.session_state[cache_key] = result
                        st.subheader("üìã Task Information")
                        st.write(f"**Description:** {result['task']['description']}")
                        st.write(f"**Status:** {result['task']['status']}")
                        st.write(f"**Created:** {result['task']['created_at']}")
                        st.subheader("üìã Execution Plan & Steps")
                        for i, step in enumerate(result['plan'], 1):
                            with st.expander(f"Step {i}: {step['step'].replace('_', ' ').title()}", expanded=True):
                                st.write(f"**Description:** {step['description']}")
                                st.write(f"**Tool:** {step['tool']}")
                                st.write(f"**Expected Outcome:** {step['expected_outcome']}")
                        st.subheader("üìä Results by Step (Detailed)")
                        for step_name, step_result in result['results'].items():
                            with st.expander(f"Results: {step_name.replace('_', ' ').title()}", expanded=True):
                                if isinstance(step_result, dict):
                                    for key, value in step_result.items():
                                        if isinstance(value, list):
                                            st.write(f"**{key}:**")
                                            for v in value:
                                                st.write(f"  - {v}")
                                        else:
                                            st.write(f"**{key}:** {value}")
                                else:
                                    st.write(step_result)
                        st.success("‚úÖ Task completed and results displayed above.")
                    except Exception as e:
                        st.error(f"Error: {str(e)}")
            else:
                st.warning("Please enter a research goal.")
        st.markdown("---")
        with st.expander("‚ÑπÔ∏è How does Agentic AI Planning work in SciSynth?", expanded=False):
            st.markdown(get_paradigm_explanation("Agentic AI"))
    else:
        st.warning("Orchestrator not initialized. Process research collection first.")

# --- SYMBOLIC & NEURO-SYMBOLIC AI SECTION ---
if page == "üî¢ Symbolic & Neuro-Symbolic AI":
    st.header("üî¢ Symbolic & Neuro-Symbolic AI: Logic, Rules, and Hybrid Reasoning")
    st.markdown("""
    This module demonstrates symbolic reasoning (logic rules, citation networks) and neuro-symbolic integration (combining neural embeddings with symbolic logic).
    """)
    st.subheader("Symbolic Reasoning: Consistency Checking & Prolog Rules")
    st.code("""
    Example Prolog Rule:
    indirect(X, Z) :- causes(X, Y), causes(Y, Z).
    """, language="prolog")
    st.write("**Python Consistency Checker Example:**")
    st.code("""
def check_consistency(hypothesis: str) -> bool:
    if any(word in hypothesis.lower() for word in ["not", "no", "null", "none"]):
        return False
    return True
    """, language="python")
    st.markdown("---")
    st.subheader("Neuro-Symbolic Integration")
    st.markdown("""
    - Combines neural embeddings (Sentence Transformers) with symbolic rules (Prolog, Python)
    - Enables hybrid reasoning for advanced research analysis
    """)
    st.info("Citation network and logical inference are visualized in the Citation Network Analysis section.")
    st.markdown("---")
    with st.expander("‚ÑπÔ∏è How does Symbolic & Neuro-Symbolic AI work in SciSynth?", expanded=False):
        st.markdown(get_paradigm_explanation("Symbolic AI") + "\n---\n" + get_paradigm_explanation("Neuro-Symbolic AI"))

# --- CAPABILITIES DASHBOARD ---
if page == "üìã Capabilities Dashboard":
    st.header("üìã AI Capabilities Dashboard")
    st.markdown("**Complete AI Technology Stack** (Professional Edition)")
    if st.session_state.orchestrator:
        capabilities = st.session_state.orchestrator.get_ai_capabilities_summary()
        for ai_type, details in capabilities.items():
            with st.expander(f"ü§ñ {ai_type.upper()} - How it works in SciSynth", expanded=False):
                st.markdown(get_paradigm_explanation(ai_type.replace('_', ' ').title()))
        st.success("üéâ **ALL AI CAPABILITIES ARE FULLY IMPLEMENTED!**")
        st.balloons()
        st.subheader("üìä Implementation Summary")
        summary_data = []
        for ai_type, details in capabilities.items():
            summary_data.append({
                "AI Type": ai_type.upper(),
                "Status": details["status"],
                "Capabilities": len(details["capabilities"]),
                "Implementation": "‚úÖ Complete"
            })
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True)
    else:
        st.warning("Orchestrator not initialized. Please process research collection first.")

# --- (Other original sections remain, but are now accessible from the sidebar as advanced modules) ---
# ... existing code ...

# Footer
st.markdown("---")
st.markdown("*SciSynth: AI Research Assistant - Helping researchers analyze and synthesize scientific literature*") 
